{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278e7451",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1 align =\"center\"> Dynamic Prompting for task completion</h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805163f0-e5fe-4753-9e38-dc0f4c85bd2c",
   "metadata": {},
   "source": [
    "Recent papers such as [Do Prompt-Based Models Really Understand the Meaning of their Prompts?](https://arxiv.org/abs/2109.01247) and [What Makes Good In-Context Examples for GPT-3?](https://aclanthology.org/2022.deelio-1.10.pdf) have shown that using dynamic set of examples instead of fixed set of examples help GPT-3 to perfom the task with higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5655453",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from requests>=2.20->openai) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: colorama in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from torch) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "     ---------------------------------------- 0.0/86.0 kB ? eta -:--:--\n",
      "     ---- ----------------------------------- 10.2/86.0 kB ? eta -:--:--\n",
      "     ---- ----------------------------------- 10.2/86.0 kB ? eta -:--:--\n",
      "     ------------- ------------------------ 30.7/86.0 kB 220.2 kB/s eta 0:00:01\n",
      "     ------------------ ------------------- 41.0/86.0 kB 219.4 kB/s eta 0:00:01\n",
      "     --------------------------- ---------- 61.4/86.0 kB 299.4 kB/s eta 0:00:01\n",
      "     -------------------------------------- 86.0/86.0 kB 373.0 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from sentence_transformers) (4.33.1)\n",
      "Requirement already satisfied: tqdm in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from sentence_transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: numpy in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from sentence_transformers) (1.25.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from sentence_transformers) (1.11.2)\n",
      "Requirement already satisfied: nltk in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from sentence_transformers) (3.8.1)\n",
      "Collecting sentencepiece (from sentence_transformers)\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
      "     ---------------------------------------- 0.0/977.5 kB ? eta -:--:--\n",
      "     --- ----------------------------------- 92.2/977.5 kB 5.5 MB/s eta 0:00:01\n",
      "     ------ ------------------------------- 174.1/977.5 kB 1.8 MB/s eta 0:00:01\n",
      "     -------- ----------------------------- 225.3/977.5 kB 2.0 MB/s eta 0:00:01\n",
      "     ------------------ ------------------- 481.3/977.5 kB 2.5 MB/s eta 0:00:01\n",
      "     -------------------------- ----------- 675.8/977.5 kB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 977.5/977.5 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from sentence_transformers) (0.16.4)\n",
      "Requirement already satisfied: filelock in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.3)\n",
      "Requirement already satisfied: fsspec in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
      "Requirement already satisfied: sympy in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.3)\n",
      "Requirement already satisfied: click in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from nltk->sentence_transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from nltk->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from torchvision->sentence_transformers) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "Building wheels for collected packages: sentence_transformers\n",
      "  Building wheel for sentence_transformers (pyproject.toml): started\n",
      "  Building wheel for sentence_transformers (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125953 sha256=2a6e8e6b3b9b021f2285053f4ed884d53762d6fefbf0a348edaef40b2b14df9f\n",
      "  Stored in directory: c:\\users\\al chat\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local\\pip\\cache\\wheels\\ff\\27\\bf\\ffba8b318b02d7f691a57084ee154e26ed24d012b0c7805881\n",
      "Successfully built sentence_transformers\n",
      "Installing collected packages: sentencepiece, sentence_transformers\n",
      "Successfully installed sentence_transformers-2.2.2 sentencepiece-0.1.99\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (1.25.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (2.14.5)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from datasets) (2.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from scikit-learn) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\trainings\\ai\\labs\\openai\\basic_samples\\.venv\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# if needed, upgrade to the latest version of the OpenAI Python library\n",
    "%pip install --upgrade openai\n",
    "%pip install --upgrade torch\n",
    "%pip install --upgrade sentence_transformers\n",
    "%pip install --upgrade numpy\n",
    "%pip install --upgrade datasets\n",
    "%pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccbb9a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os module & the OpenAI Python library for calling the OpenAI API\n",
    "# please make sure you have installed required libraries via pip install -r requirements.txt\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c650b9-ae37-46cb-a47a-2386f096adb7",
   "metadata": {},
   "source": [
    "## Dataset Summary\n",
    "\n",
    "The Text REtrieval Conference (TREC) Question Classification dataset is a dataset for question classification consisting of open-domain, fact-based questions divided into broad semantic categories. It contains 5500 labeled questions in training set and another 500 for test set.\n",
    "\n",
    "The dataset has 6 coarse class labels and 50 fine class labels. Average length of each sentence is 10, vocabulary size of 8700."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceaaad07-d9e2-44e0-88aa-b72c9560ed10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83869d1d5bef4a8f84fa0587824519af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656ce55fe05a4328ab3524da511c5d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826a3514ef88420e81b06747037fc830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22aae09db1c64b3082ff2a16e37f5566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee434322d2ee490394b505fb72e5e3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/336k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72458ff1b6f34a76ad6330e9f98a6a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/23.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e4b1379c054dd19745002e26f2589f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5452 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f5200df985472e95ac2a34f89314bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset from Huggingface's dataset library\n",
    "dataset = load_dataset(\"trec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a769332c-077c-49d1-81c0-88d6ee87daa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'coarse_label', 'fine_label'],\n",
       "        num_rows: 5452\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'coarse_label', 'fine_label'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cbbcf88-3d33-4c36-b4a8-399b11b8ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the text and label column\n",
    "label_type = 'coarse_label'\n",
    "text_key = \"text\"\n",
    "# create mapping of ids2class and class2id\n",
    "id2class = dict((i, label) for i, label in enumerate(dataset['train'].features[label_type].names))\n",
    "class2id = dict((label, i) for i, label in enumerate(dataset['train'].features[label_type].names))\n",
    "# create a dictionary with classes as key and containing all the training examples within that class\n",
    "class2TrainDataset = dict((label, []) for label in dataset['train'].features[label_type].names)\n",
    "for example in dataset['train']:\n",
    "    label = id2class[example[label_type]]\n",
    "    class2TrainDataset[label].append(example[text_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4ea26-5071-41d8-a142-3426ada87f58",
   "metadata": {},
   "source": [
    "# Task Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0442beee-70fb-4555-9e11-654fe0fa9621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a prompt for asking LLM to perform a task\n",
    "task_prompt = \"As a Question Answering agent, your goal is to categorize questions into different semantic classes that impose constraints on potential answers, so that they can be utilized in later stages of the question answering process.\\nFollowing are the semantic classes: [\"\n",
    "task_prompt += \", \".join([label for label in class2TrainDataset]) + \"]\"\n",
    "# a prompt for asking LLM to generate the output for current task\n",
    "query_prompt = \"\\nClassify the following question into one of the above classes. Please answer in a single word.\\nquestion: \"\n",
    "answer_prompt = \"\\noutput: \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c2541-4a80-4dc5-b773-bc85452aad7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25a92251-1fab-46dc-9e98-29b7fb51a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config values\n",
    "with open(r'config.json') as config_file:\n",
    "    config_details = json.load(config_file)\n",
    "    \n",
    "# Setting up the deployment name\n",
    "# Recommended models: text-davinci-002 or text-davinci-003 (deployment name =\"dv003\")\n",
    "model_name = 'demolabsmsneards'\n",
    "\n",
    "# This is set to `azure`\n",
    "openai.api_type = \"azure\"\n",
    "\n",
    "# The API key for your Azure OpenAI resource.\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai.api_base = config_details['OPENAI_API_BASE']\n",
    "\n",
    "# Currently OPENAI API have the following versions available: 2022-12-01\n",
    "openai.api_version = config_details['OPENAI_API_VERSION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a03c4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text completion using GPT\n",
    "def trim_text(text):\n",
    "    return text.strip().strip('\\n').strip('\\\\n')\n",
    "\n",
    "def generate_using_gpt(prompt):\n",
    "    generated_sentence = \"\"\n",
    "    try:\n",
    "        # Create a completion for the provided prompt and parameters\n",
    "        # To know more about the parameters, checkout this documentation: https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference\n",
    "        response = openai.Completion.create(\n",
    "            engine=model_name,\n",
    "            prompt=prompt, \n",
    "            max_tokens=3,\n",
    "            temperature=0,\n",
    "            top_p=1,\n",
    "            stop=None,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0.0)\n",
    "        \n",
    "        choices = response.get(\"choices\", \"\")\n",
    "        if len(choices) == 0 or \"text\" not in choices[0]:\n",
    "            print(\"Text not generated properly\")\n",
    "        generated_sentence = choices[0][\"text\"].lstrip('\\\\n').rstrip('\\\\n').lstrip('\\n\\n').rstrip('\\n\\n').lstrip('\\n').rstrip('\\n')\n",
    "        \n",
    "    except openai.error.APIError as e:\n",
    "        # Handle API error here, e.g. retry or log\n",
    "        print(f\"OpenAI API returned an API Error: {e}\")\n",
    "\n",
    "    except openai.error.AuthenticationError as e:\n",
    "        # Handle Authentication error here, e.g. invalid API key\n",
    "        print(f\"OpenAI API returned an Authentication Error: {e}\")\n",
    "\n",
    "    except openai.error.APIConnectionError as e:\n",
    "        # Handle connection error here\n",
    "        print(f\"Failed to connect to OpenAI API: {e}\")\n",
    "\n",
    "    except openai.error.InvalidRequestError as e:\n",
    "        # Handle connection error here\n",
    "        print(f\"Invalid Request Error: {e}\")\n",
    "        \n",
    "    except openai.error.RateLimitError as e:\n",
    "        # Handle rate limit error\n",
    "        print(f\"OpenAI API request exceeded rate limit: {e}\")\n",
    "\n",
    "    except openai.error.ServiceUnavailableError as e:\n",
    "        # Handle Service Unavailable error\n",
    "        print(f\"Service Unavailable: {e}\")\n",
    "\n",
    "    except openai.error.Timeout as e:\n",
    "        # Handle request timeout\n",
    "        print(f\"Request timed out: {e}\")\n",
    "    return generated_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d2126-c1b8-4942-ae60-e70f93265a3d",
   "metadata": {},
   "source": [
    "# Zero-shot Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057efe38-749e-4443-a565-2163506eac1d",
   "metadata": {},
   "source": [
    "### Example of the zero-shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ea8ae71-68b9-41b0-a3a6-3c92a3f3dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a Question Answering agent, your goal is to categorize questions into different semantic classes that impose constraints on potential answers, so that they can be utilized in later stages of the question answering process.\n",
      "Following are the semantic classes: [ABBR, ENTY, DESC, HUM, LOC, NUM]\n",
      "Classify the following question into one of the above classes. Please answer in a single word.\n",
      "question: How far is it from Denver to Aspen ?\n",
      "output: \n"
     ]
    }
   ],
   "source": [
    "zeroshot_prompt = task_prompt +  query_prompt + dataset['test'][0][text_key] + answer_prompt\n",
    "print(zeroshot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eee4a12e-3ce0-457a-ab23-c94c42071b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt without any examples from the training dataset\n",
    "labels = []\n",
    "predictions = []\n",
    "for example in dataset['test']:\n",
    "    zeroshot_prompt = task_prompt +  query_prompt + example[text_key] + answer_prompt\n",
    "    pred = generate_using_gpt(zeroshot_prompt)\n",
    "    pred=trim_text(pred)\n",
    "    labels.append(example[label_type])\n",
    "    if pred not in class2id:\n",
    "        predictions.append(-1)\n",
    "    else:\n",
    "        predictions.append(class2id[pred])\n",
    "        \n",
    "report = classification_report(labels, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aba42eb-7f20-45ea-a893-38cde49f6859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      1.00      0.82         9\n",
      "           1       0.28      0.68      0.40        94\n",
      "           2       0.77      0.14      0.24       138\n",
      "           3       0.82      0.22      0.34        65\n",
      "           4       0.66      0.90      0.76        81\n",
      "           5       0.81      0.78      0.80       113\n",
      "\n",
      "    accuracy                           0.54       500\n",
      "   macro avg       0.67      0.62      0.56       500\n",
      "weighted avg       0.68      0.54      0.51       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258912ca-9593-4fe0-9627-44dfcd93753a",
   "metadata": {},
   "source": [
    "# Few-shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6269709e-a4e9-4a33-a4ff-96c6edc38204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to selection few examples in each of the classes from the training dataset\n",
    "def generateFewshotPrompt(class2TrainDataset, N=3):\n",
    "    fewshot_prompt = \"\\nFollowing are some examples.\"\n",
    "    for label in class2TrainDataset:\n",
    "        for example in class2TrainDataset[label][:N]:\n",
    "            fewshot_prompt += \"\\nquestion: \" + example\n",
    "            fewshot_prompt += \"\\noutput: \" + label\n",
    "    return fewshot_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c6777-da11-4d51-9cb8-df07eaf57c7b",
   "metadata": {},
   "source": [
    "### Example of the few-shot prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68c25be0-afda-4a45-bc67-ba2962ad32de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a Question Answering agent, your goal is to categorize questions into different semantic classes that impose constraints on potential answers, so that they can be utilized in later stages of the question answering process.\n",
      "Following are the semantic classes: [DESC, ENTY, ABBR, HUM, NUM, LOC]\n",
      "Following are some examples.\n",
      "question: How did serfdom develop in and then leave Russia ?\n",
      "output: DESC\n",
      "question: What films featured the character Popeye Doyle ?\n",
      "output: ENTY\n",
      "question: What is the full form of .com ?\n",
      "output: ABBR\n",
      "question: What contemptible scoundrel stole the cork from my lunch ?\n",
      "output: HUM\n",
      "question: When was Ozzy Osbourne born ?\n",
      "output: NUM\n",
      "question: What sprawling U.S. state boasts the most airports ?\n",
      "output: LOC\n",
      "Classify the following question into one of the above classes. Please answer in a single word.\n",
      "question: How far is it from Denver to Aspen ?\n",
      "output: \n"
     ]
    }
   ],
   "source": [
    "# prompt with one example in each of the classes\n",
    "fewshot_examples = generateFewshotPrompt(class2TrainDataset, N=1)\n",
    "fewshot_prompt = task_prompt +  fewshot_examples + query_prompt + dataset['test'][0][text_key] + answer_prompt\n",
    "print(fewshot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38f65537-04e7-4dee-a40d-a28973e27dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt is created by adding one example in each of the classes \n",
    "labels = []\n",
    "predictions = []\n",
    "for example in dataset['test']:\n",
    "    fewshot_prompt = task_prompt + fewshot_examples + query_prompt + example[text_key] + answer_prompt\n",
    "    pred = generate_using_gpt(fewshot_prompt)\n",
    "    pred=trim_text(pred)\n",
    "    labels.append(example[label_type])\n",
    "    if pred not in class2id:\n",
    "        predictions.append(-1)\n",
    "    else:\n",
    "        predictions.append(class2id[pred])\n",
    "        \n",
    "report = classification_report(labels, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "724a5041-ded3-4fe9-9db9-d47556b6ba10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         9\n",
      "           1       0.39      0.74      0.51        94\n",
      "           2       0.87      0.42      0.57       138\n",
      "           3       0.91      0.45      0.60        65\n",
      "           4       0.95      0.88      0.91        81\n",
      "           5       0.84      1.00      0.91       113\n",
      "\n",
      "    accuracy                           0.70       500\n",
      "   macro avg       0.78      0.75      0.73       500\n",
      "weighted avg       0.79      0.70      0.70       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1d828-72fa-4540-8995-157cce2c82c8",
   "metadata": {},
   "source": [
    "# Extract Embeddings for Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf81306b-1e58-49a1-913b-2569328d609c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading Sentence Transformer based model\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=device)\n",
    "\n",
    "# extract embeddings for a set of examples\n",
    "def ExtractEmbeddings(examples):\n",
    "    embedding_ls = []\n",
    "    for example in examples:\n",
    "        embedding = model.encode(example)     \n",
    "        embedding_ls.append(embedding)\n",
    "    return embedding_ls\n",
    "\n",
    "# extract embeddings for all the training examples\n",
    "class2TrainDatasetWithEmbedding = {}\n",
    "for label in class2TrainDataset:\n",
    "    embeddings = ExtractEmbeddings(class2TrainDataset[label])\n",
    "    class2TrainDatasetWithEmbedding[label] = [class2TrainDataset[label], embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8c8ce6-60a0-4abc-8d00-5cbc82ba6c00",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dynamic Few-shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a42cb4a5-fa35-441e-96ec-76edcd5ae4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract similar queries for a given input text from each of the classes\n",
    "def getSimilarExamples(input_text, dataset, dataset_embedding):\n",
    "    input_embedding = model.encode(input_text)\n",
    "    sim_score = util.dot_score(input_embedding, dataset_embedding)[0]\n",
    "    topN_ids = np.argsort(-sim_score)\n",
    "    return [dataset[i] for i in topN_ids]\n",
    "    \n",
    "def getClasswiseSimilarExamples(input_text, class2TrainDatasetWithEmbedding):\n",
    "    classwiseSimilarExamples = {}\n",
    "    for label in class2TrainDataset:\n",
    "        similarExamples = getSimilarExamples(input_text, class2TrainDatasetWithEmbedding[label][0], class2TrainDatasetWithEmbedding[label][1])\n",
    "        classwiseSimilarExamples[label] = similarExamples\n",
    "    return classwiseSimilarExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78d83dfc-fa32-4d86-a730-6a63ff3578dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a prompt with similar examples in each of the classes\n",
    "def generateDynamicPrompt(input_text, class2TrainDatasetWithEmbedding, N=3):\n",
    "    classwiseSimilarExamples = getClasswiseSimilarExamples(input_text, class2TrainDatasetWithEmbedding)\n",
    "    dynamic_prompt = \"\\nFollowing are some examples.\"\n",
    "    for label in classwiseSimilarExamples:\n",
    "        for example in classwiseSimilarExamples[label][:N]:\n",
    "            dynamic_prompt += \"\\nquestion: \" + example\n",
    "            dynamic_prompt += \"\\noutput: \" + label\n",
    "    return dynamic_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456e460c-6734-47fe-9ca5-3d2f78eb144c",
   "metadata": {},
   "source": [
    "### Example of the dynamic prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c6c4482-a00f-4a0c-8809-531916ec239e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a Question Answering agent, your goal is to categorize questions into different semantic classes that impose constraints on potential answers, so that they can be utilized in later stages of the question answering process.\n",
      "Following are the semantic classes: [DESC, ENTY, ABBR, HUM, NUM, LOC]\n",
      "Following are some examples.\n",
      "question: Why is the mile 528 feet ?\n",
      "output: DESC\n",
      "question: What race is 1 , 137 miles long ?\n",
      "output: ENTY\n",
      "question: What do the letters D.C. stand for in Washington , D.C. ?\n",
      "output: ABBR\n",
      "question: Who lives at 39 Stone Canyon Way ?\n",
      "output: HUM\n",
      "question: How high is the city of Denver ?\n",
      "output: NUM\n",
      "question: What Colorado city owns its own glacier ?\n",
      "output: LOC\n",
      "Classify the following question into one of the above classes. Please answer in a single word.\n",
      "question: How far is it from Denver to Aspen ?\n",
      "output: \n"
     ]
    }
   ],
   "source": [
    "# dynamic prompt with one similar example in each of the classes\n",
    "fewshot_examples = generateDynamicPrompt(dataset['test'][0][text_key], class2TrainDatasetWithEmbedding, N=1)\n",
    "dynamic_prompt = task_prompt + fewshot_examples + query_prompt + dataset['test'][0][text_key] + answer_prompt\n",
    "print(dynamic_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc17023f-678b-4386-b7e7-541282b30f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "predictions = []\n",
    "for example in dataset['test']:\n",
    "    fewshot_examples = generateDynamicPrompt(example[text_key], class2TrainDatasetWithEmbedding, N=1)\n",
    "    dynamic_prompt = task_prompt + fewshot_examples + query_prompt + example[text_key] + answer_prompt\n",
    "    pred = generate_using_gpt3(dynamic_prompt)\n",
    "    pred=trim_text(pred)\n",
    "    labels.append(example[label_type])\n",
    "    if pred not in class2id:\n",
    "        predictions.append(-1)\n",
    "    else:\n",
    "        predictions.append(class2id[pred])\n",
    "        \n",
    "report = classification_report(labels, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "826584d8-8223-4f88-89cc-65181212a1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      1.00      0.82         9\n",
      "           1       0.61      0.80      0.69        94\n",
      "           2       0.88      0.68      0.77       138\n",
      "           3       0.95      0.88      0.91        65\n",
      "           4       0.93      0.86      0.90        81\n",
      "           5       0.90      0.98      0.94       113\n",
      "\n",
      "    accuracy                           0.83       500\n",
      "   macro avg       0.83      0.87      0.84       500\n",
      "weighted avg       0.85      0.83      0.83       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
