{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUlgyRebGtNg"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da7fjJn2JDkf"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vzabQYDmIEYR"
      },
      "outputs": [],
      "source": [
        "import pinecone\n",
        "from openai.embeddings_utils import get_embedding\n",
        "from tqdm import tqdm\n",
        "import docx\n",
        "import os\n",
        "import openai\n",
        "import json\n",
        "import pandas as pd \n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "# The API key for your Azure OpenAI resource.\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc9vsSXNMsa3"
      },
      "source": [
        "# Mount drive a specify the folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rR7IbBDSMwaZ"
      },
      "outputs": [],
      "source": [
        "docs_path = \"./data/Function calling.docx\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7C2eh4TVOPU"
      },
      "source": [
        "# Parse Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PTDRN9P5T2B9"
      },
      "outputs": [],
      "source": [
        "text_chunks = []\n",
        "doc = docx.Document(docs_path)\n",
        "for para in doc.paragraphs:\n",
        "    text_chunks.append(para.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4aFLhBnbZ8-7"
      },
      "outputs": [],
      "source": [
        "# remove all chunks shorter than 10 words and strip the rest\n",
        "text_chunks = [string.strip().strip('\\n') for string in text_chunks if len(string.split()) >= 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load config values\n",
        "with open(r'../config.json') as config_file:\n",
        "    config_details = json.load(config_file)\n",
        "    \n",
        "# Setting up the deployment name\n",
        "deployment_name = config_details['EMBEDDINGS_MODEL']\n",
        "\n",
        "# This is set to `azure`\n",
        "openai.api_type = \"azure\"\n",
        "\n",
        "# The API key for your Azure OpenAI resource.\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
        "openai.api_base = config_details['OPENAI_API_BASE']\n",
        "\n",
        "# Currently OPENAI API have the following versions available: 2022-12-01\n",
        "openai.api_version = config_details['OPENAI_API_VERSION']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB7bq2ucbGeD"
      },
      "source": [
        "# Generate embeddigns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "UOK6ovzbbFNs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:11<00:00,  5.64s/it]\n"
          ]
        }
      ],
      "source": [
        "chunks_with_embeddigns = []\n",
        "i = 1\n",
        "for chunk in tqdm(text_chunks):\n",
        "  embedding = get_embedding(chunk, engine=deployment_name)\n",
        "+(+i)\n",
        "chunks_with_embeddigns.append({\"id\":i, , \"values\": embedding})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Column `metadata` is expected to be a dictionary, found <class 'str'>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m index\u001b[39m.\u001b[39;49mupsert(chunks_with_embeddigns)\n",
            "File \u001b[1;32md:\\Trainings\\AI\\labs\\openai\\Basic_Samples\\.venv\\Lib\\site-packages\\pinecone\\core\\utils\\error_handling.py:17\u001b[0m, in \u001b[0;36mvalidate_and_convert_errors.<locals>.inner_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     15\u001b[0m Config\u001b[39m.\u001b[39mvalidate()  \u001b[39m# raises exceptions in case of invalid config\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     18\u001b[0m \u001b[39mexcept\u001b[39;00m MaxRetryError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     19\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ProtocolError):\n",
            "File \u001b[1;32md:\\Trainings\\AI\\labs\\openai\\Basic_Samples\\.venv\\Lib\\site-packages\\pinecone\\index.py:150\u001b[0m, in \u001b[0;36mIndex.upsert\u001b[1;34m(self, vectors, namespace, batch_size, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39masync_req is not supported when batch_size is provided.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    146\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mTo upsert in parallel, please follow: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    147\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mhttps://docs.pinecone.io/docs/insert-data#sending-upserts-in-parallel\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    149\u001b[0m \u001b[39mif\u001b[39;00m batch_size \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_upsert_batch(vectors, namespace, _check_type, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    152\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_size, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m batch_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    153\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mbatch_size must be a positive integer\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32md:\\Trainings\\AI\\labs\\openai\\Basic_Samples\\.venv\\Lib\\site-packages\\pinecone\\index.py:239\u001b[0m, in \u001b[0;36mIndex._upsert_batch\u001b[1;34m(self, vectors, namespace, _check_type, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[39mreturn\u001b[39;00m _dict_to_vector(item)\n\u001b[0;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid vector value passed: cannot interpret type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(item)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    237\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vector_api\u001b[39m.\u001b[39mupsert(\n\u001b[0;32m    238\u001b[0m     UpsertRequest(\n\u001b[1;32m--> 239\u001b[0m         vectors\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(_vector_transform, vectors)),\n\u001b[0;32m    240\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs_dict,\n\u001b[0;32m    241\u001b[0m         _check_type\u001b[39m=\u001b[39m_check_type,\n\u001b[0;32m    242\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _OPENAPI_ENDPOINT_PARAMS}\n\u001b[0;32m    243\u001b[0m     ),\n\u001b[0;32m    244\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m _OPENAPI_ENDPOINT_PARAMS}\n\u001b[0;32m    245\u001b[0m )\n",
            "File \u001b[1;32md:\\Trainings\\AI\\labs\\openai\\Basic_Samples\\.venv\\Lib\\site-packages\\pinecone\\index.py:234\u001b[0m, in \u001b[0;36mIndex._upsert_batch.<locals>._vector_transform\u001b[1;34m(item)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[39mreturn\u001b[39;00m Vector(\u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39mid\u001b[39m, values\u001b[39m=\u001b[39mvalues, metadata\u001b[39m=\u001b[39mmetadata \u001b[39mor\u001b[39;00m {}, _check_type\u001b[39m=\u001b[39m_check_type)\n\u001b[0;32m    233\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, Mapping):\n\u001b[1;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m _dict_to_vector(item)\n\u001b[0;32m    235\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid vector value passed: cannot interpret type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(item)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32md:\\Trainings\\AI\\labs\\openai\\Basic_Samples\\.venv\\Lib\\site-packages\\pinecone\\index.py:208\u001b[0m, in \u001b[0;36mIndex._upsert_batch.<locals>._dict_to_vector\u001b[1;34m(item)\u001b[0m\n\u001b[0;32m    206\u001b[0m     metadata \u001b[39m=\u001b[39m item\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(metadata, Mapping):\n\u001b[1;32m--> 208\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn `metadata` is expected to be a dictionary, found \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(metadata)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(item[\u001b[39m'\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m'\u001b[39m], np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m    211\u001b[0m     upsert_numpy_deprecation_notice(\u001b[39m\"\u001b[39m\u001b[39mDeprecated type passed in \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: Column `metadata` is expected to be a dictionary, found <class 'str'>"
          ]
        }
      ],
      "source": [
        "    index.upsert(chunks_with_embeddigns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "xchunks_with_embeddigns = pd.DataFrame(chunks_with_embeddigns)\n",
        "\n",
        "chunks_with_embeddigns = xchunks_with_embeddigns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "text         In an API call, you can describe functions to ...\n",
              "embedding    [-0.02345113828778267, 0.005468207411468029, 0...\n",
              "Name: 0, dtype: object"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks_with_embeddigns.iloc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc_xdNnAduLx"
      },
      "source": [
        "# Upload to Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "L1jcoDObeLO-"
      },
      "outputs": [],
      "source": [
        "pinecone.init(\n",
        "    api_key=\"f5444e56-58db-42db-afd6-d4bd9b2cb40c\",\n",
        "    environment=\"asia-southeast1-gcp-free\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uq6ABbtVdGOS"
      },
      "outputs": [],
      "source": [
        "# create or connect to index\n",
        "index_name = \"function-info-2023\"\n",
        "\n",
        "index = pinecone.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:17<00:00,  8.85s/it]\n"
          ]
        }
      ],
      "source": [
        "chunks_with_embeddigns = []\n",
        "for chunk in tqdm(text_chunks):\n",
        "  embedding = get_embedding(chunk, engine=deployment_name)\n",
        "  index.upsert()\n",
        "\n",
        "\n",
        "\n",
        "chunks_with_embeddigns.append({\"id\": chunk, \"value\": embedding})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Column `metadata` is expected to be a dictionary, found <class 'str'>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m index\u001b[39m.\u001b[39;49mupsert(chunks_with_embeddigns)\n",
            "File \u001b[1;32md:\\Trainings\\AI\\labs\\openai\\Basic_Samples\\.venv\\Lib\\site-packages\\pinecone\\core\\utils\\error_handling.py:17\u001b[0m, in \u001b[0;36mvalidate_and_convert_errors.<locals>.inner_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     15\u001b[0m Config\u001b[39m.\u001b[39mvalidate()  \u001b[39m# raises exceptions in case of invalid config\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     18\u001b[0m \u001b[39mexcept\u001b[39;00m MaxRetryError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     19\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ProtocolError):\n",
            "File \u001b[1;32md:\\Trainings\\AI\\labs\\openai\\Basic_Samples\\.venv\\Lib\\site-packages\\pinecone\\index.py:150\u001b[0m, in \u001b[0;36mIndex.upsert\u001b[1;34m(self, vectors, namespace, batch_size, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39masync_req is not supported when batch_size is provided.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    146\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mTo upsert in parallel, please follow: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    147\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mhttps://docs.pinecone.io/docs/insert-data#sending-upserts-in-parallel\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    149\u001b[0m \u001b[39mif\u001b[39;00m batch_size \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_upsert_batch(vectors, namespace, _check_type, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    152\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_size, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m batch_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    153\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mbatch_size must be a positive integer\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32md:\\Trainings\\AI\\labs\\openai\\Basic_Samples\\.venv\\Lib\\site-packages\\pinecone\\index.py:239\u001b[0m, in \u001b[0;36mIndex._upsert_batch\u001b[1;34m(self, vectors, namespace, _check_type, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[39mreturn\u001b[39;00m _dict_to_vector(item)\n\u001b[0;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid vector value passed: cannot interpret type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(item)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    237\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vector_api\u001b[39m.\u001b[39mupsert(\n\u001b[0;32m    238\u001b[0m     UpsertRequest(\n\u001b[1;32m--> 239\u001b[0m         vectors\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(_vector_transform, vectors)),\n\u001b[0;32m    240\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs_dict,\n\u001b[0;32m    241\u001b[0m         _check_type\u001b[39m=\u001b[39m_check_type,\n\u001b[0;32m    242\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _OPENAPI_ENDPOINT_PARAMS}\n\u001b[0;32m    243\u001b[0m     ),\n\u001b[0;32m    244\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m _OPENAPI_ENDPOINT_PARAMS}\n\u001b[0;32m    245\u001b[0m )\n",
            "File \u001b[1;32md:\\Trainings\\AI\\labs\\openai\\Basic_Samples\\.venv\\Lib\\site-packages\\pinecone\\index.py:234\u001b[0m, in \u001b[0;36mIndex._upsert_batch.<locals>._vector_transform\u001b[1;34m(item)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[39mreturn\u001b[39;00m Vector(\u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39mid\u001b[39m, values\u001b[39m=\u001b[39mvalues, metadata\u001b[39m=\u001b[39mmetadata \u001b[39mor\u001b[39;00m {}, _check_type\u001b[39m=\u001b[39m_check_type)\n\u001b[0;32m    233\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, Mapping):\n\u001b[1;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m _dict_to_vector(item)\n\u001b[0;32m    235\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid vector value passed: cannot interpret type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(item)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32md:\\Trainings\\AI\\labs\\openai\\Basic_Samples\\.venv\\Lib\\site-packages\\pinecone\\index.py:208\u001b[0m, in \u001b[0;36mIndex._upsert_batch.<locals>._dict_to_vector\u001b[1;34m(item)\u001b[0m\n\u001b[0;32m    206\u001b[0m     metadata \u001b[39m=\u001b[39m item\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(metadata, Mapping):\n\u001b[1;32m--> 208\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn `metadata` is expected to be a dictionary, found \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(metadata)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(item[\u001b[39m'\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m'\u001b[39m], np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m    211\u001b[0m     upsert_numpy_deprecation_notice(\u001b[39m\"\u001b[39m\u001b[39mDeprecated type passed in \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: Column `metadata` is expected to be a dictionary, found <class 'str'>"
          ]
        }
      ],
      "source": [
        "index.upsert(chunks_with_embeddigns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Pinecone.__init__() got an unexpected keyword argument 'embeddings'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m chunks_with_embeddigns \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m insert_embeddings(index,tqdm(text_chunks) )\n\u001b[0;32m      4\u001b[0m   \u001b[39m#embedding = get_embedding(chunk, engine=deployment_name)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m   \u001b[39m#chunks_with_embeddigns.append({\"text\": chunk, \"embedding\": embedding})\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[9], line 8\u001b[0m, in \u001b[0;36minsert_embeddings\u001b[1;34m(indx, chunks)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mopenai\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[0;32m      6\u001b[0m embeddings \u001b[39m=\u001b[39m OpenAIEmbeddings()\n\u001b[1;32m----> 8\u001b[0m vector_store \u001b[39m=\u001b[39m Pinecone(indx, embeddings\u001b[39m=\u001b[39;49membeddings)\n\u001b[0;32m     10\u001b[0m \u001b[39m# Batch insert the chunks into the vector store\u001b[39;00m\n\u001b[0;32m     11\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m  \u001b[39m# Define your preferred batch size\u001b[39;00m\n",
            "\u001b[1;31mTypeError\u001b[0m: Pinecone.__init__() got an unexpected keyword argument 'embeddings'"
          ]
        }
      ],
      "source": [
        "chunks_with_embeddigns = []\n",
        "\n",
        "insert_embeddings(index,tqdm(text_chunks) )\n",
        "  #embedding = get_embedding(chunk, engine=deployment_name)\n",
        "  #chunks_with_embeddigns.append({\"text\": chunk, \"embedding\": embedding})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "0dSrfmyUe0p6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "string indices must be integers, not 'str'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[41], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m i_end \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(i\u001b[39m+\u001b[39mbatch_size, \u001b[39mlen\u001b[39m(chunks_with_embeddigns))\n\u001b[0;32m      6\u001b[0m \u001b[39m# get batch meta\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m text_batch \u001b[39m=\u001b[39m [item[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m item \u001b[39min\u001b[39;49;00m data_batch]\n\u001b[0;32m      8\u001b[0m \u001b[39m# get ids\u001b[39;00m\n\u001b[0;32m      9\u001b[0m ids_batch \u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39m(n) \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(i, i_end)]\n",
            "Cell \u001b[1;32mIn[41], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m i_end \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(i\u001b[39m+\u001b[39mbatch_size, \u001b[39mlen\u001b[39m(chunks_with_embeddigns))\n\u001b[0;32m      6\u001b[0m \u001b[39m# get batch meta\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m text_batch \u001b[39m=\u001b[39m [item[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m data_batch]\n\u001b[0;32m      8\u001b[0m \u001b[39m# get ids\u001b[39;00m\n\u001b[0;32m      9\u001b[0m ids_batch \u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39m(n) \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(i, i_end)]\n",
            "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
          ]
        }
      ],
      "source": [
        "batch_size = 2  # process everything in batches of 64\n",
        "for i in tqdm(range(0, len(chunks_with_embeddigns), batch_size)):\n",
        "    data_batch = chunks_with_embeddigns.iloc[i: i+batch_size]\n",
        "    # set end position of batch\n",
        "    i_end = min(i+batch_size, len(chunks_with_embeddigns))\n",
        "    # get batch meta\n",
        "    text_batch = [item['text'] for item in data_batch]\n",
        "    # get ids\n",
        "    ids_batch = [str(n) for n in range(i, i_end)]\n",
        "    # get embeddings\n",
        "    embeds = [item['embedding'] for item in data_batch]\n",
        "    # prep metadata and upsert batch\n",
        "    meta = [{'text': text_batch} for text_batch in zip(text_batch)] # you can add more fields here\n",
        "    to_upsert = zip(ids_batch, embeds, meta)\n",
        "    # upsert to Pinecone\n",
        "    index.upsert(vectors=list(to_upsert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWVFk3fyGzwy"
      },
      "source": [
        "# Query Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MawACaJnG0we"
      },
      "outputs": [],
      "source": [
        "def search_docs(query):\n",
        "  xq = openai.Embedding.create(input=query, engine=\"text-embedding-ada-002\")['data'][0]['embedding']\n",
        "  res = index.query([xq], top_k=5, include_metadata=True)\n",
        "  chosen_text = []\n",
        "  for match in res['matches']:\n",
        "    chosen_text = match['metadata']\n",
        "  return res['matches']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Vnoi_NaHSu-"
      },
      "outputs": [],
      "source": [
        "matches = search_docs(\"What are some predictions for tiktok?\")\n",
        "for match in matches:\n",
        "    print(f\"{match['score']:.2f}: {match['metadata']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9C11iMnH8ux"
      },
      "source": [
        "# Construct Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhxkkHunIA38"
      },
      "outputs": [],
      "source": [
        "def construct_prompt(query):\n",
        "  matches = search_docs(query)\n",
        "\n",
        "  chosen_text = []\n",
        "  for match in matches:\n",
        "    chosen_text.append(match['metadata']['text'])\n",
        "\n",
        "  prompt = \"\"\"Answer the question as truthfully as possible using the context below, and if the answer is no within the context, say 'I don't know.'\"\"\"\n",
        "  prompt += \"\\n\\n\"\n",
        "  prompt += \"Context: \" + \"\\n\".join(chosen_text)\n",
        "  prompt += \"\\n\\n\"\n",
        "  prompt += \"Question: \" + query\n",
        "  prompt += \"\\n\"\n",
        "  prompt += \"Answer: \"\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXMXg_JHIh05"
      },
      "source": [
        "# Run the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAVPi_p_MyuS"
      },
      "outputs": [],
      "source": [
        "def answer_question(query):\n",
        "  prompt = construct_prompt(query)\n",
        "  res = openai.Completion.create(\n",
        "      prompt=prompt,\n",
        "      model=\"text-davinci-003\",\n",
        "      max_tokens=500,\n",
        "      temperature=0.0,\n",
        "  )\n",
        "\n",
        "  return res.choices[0].message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlrzWiqQIkGs"
      },
      "outputs": [],
      "source": [
        "print(answer_question(\"What will be the top platform in 2023?\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
